{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d923c33a",
   "metadata": {},
   "source": [
    "## onnx library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748cf6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import cv2\n",
    "import argparse\n",
    "import onnxruntime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8a715b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./trt_inference/')\n",
    "sys.path.append('./onnx_inference/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d60ccd",
   "metadata": {},
   "source": [
    "### tensorrt library import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "639f56ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trt_inference.yolo_face_trt_inference import Predictor\n",
    "from onnx_inference.yolo_pose_onnx_inference import model_inference_image_list_wo_nms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9d857",
   "metadata": {},
   "source": [
    "### export onnx and tensorrt (without nms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc04a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build onnx file (32bit)\n",
    "#!python models/export.py --weights ./yolov7-tiny-face.pt --grid --simplify\n",
    "# build onnx file to trt file (32bit)\n",
    "#!python models/export_tensorrt.py -o ./yolov7-tiny-face_wo_nms.onnx -e ./yolov7-tiny-face_wo_nms.trt -p fp32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c128093a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/1:   0%|                                                                                        | 0/1 [00:00<?, ?it/s]/home/pdh/torch/YOLO/yolov7-face/onnx_inference/yolo_pose_onnx_inference.py:164: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  det_bboxes = boxes_xyxy[keep]\n",
      "/home/pdh/torch/YOLO/yolov7-face/onnx_inference/yolo_pose_onnx_inference.py:165: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  det_scores = scores[keep]\n",
      "/home/pdh/torch/YOLO/yolov7-face/onnx_inference/yolo_pose_onnx_inference.py:166: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  kpts = keypoints[keep]\n",
      "0/1:   0%|                                                                                        | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original shape :  (25200, 4) (25200, 15) (25200, 1) 74\n",
      "[[[0.54760987]]\n",
      "\n",
      " [[0.54760987]]\n",
      "\n",
      " [[0.54760987]]\n",
      "\n",
      " [[0.54760987]]\n",
      "\n",
      " [[0.54760987]]\n",
      "\n",
      " [[0.54760987]]\n",
      "\n",
      " [[0.54760987]]\n",
      "\n",
      " [[0.54760987]]\n",
      "\n",
      " [[0.54760987]]\n",
      "\n",
      " [[0.54760987]]]\n",
      "nms's outputs :  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './onnx_output.txt/22_Picnic_Picnic_22_10.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m trt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./yolov7-tiny-face_wo_nms.trt\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/images/22_Picnic_Picnic_22_10.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel_inference_image_list_wo_nms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monnx_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0039\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./onnx_output.png\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/torch/YOLO/yolov7-face/onnx_inference/yolo_pose_onnx_inference.py:66\u001b[0m, in \u001b[0;36mmodel_inference_image_list_wo_nms\u001b[0;34m(model_path, img_path, mean, scale, dst_path)\u001b[0m\n\u001b[1;32m     64\u001b[0m output \u001b[38;5;241m=\u001b[39m model_inference(model_path, \u001b[38;5;28minput\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     65\u001b[0m dst_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(dst_path, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(img_file))\n\u001b[0;32m---> 66\u001b[0m \u001b[43mpost_process_wo_nms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/torch/YOLO/yolov7-face/onnx_inference/yolo_pose_onnx_inference.py:170\u001b[0m, in \u001b[0;36mpost_process_wo_nms\u001b[0;34m(img_file, dst_file, predictions, ratio, score_threshold)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m#To generate color based on det_label, to look into the codebase of Tensorflow object detection api.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m dst_txt_file \u001b[38;5;241m=\u001b[39m dst_file\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtxt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdst_txt_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(det_bboxes)):\n\u001b[1;32m    172\u001b[0m     det_bbox \u001b[38;5;241m=\u001b[39m det_bboxes[idx]\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './onnx_output.txt/22_Picnic_Picnic_22_10.jpg'"
     ]
    }
   ],
   "source": [
    "onnx_path = './yolov7-tiny-face_wo_nms.onnx'\n",
    "trt_path = './yolov7-tiny-face_wo_nms.trt'\n",
    "img_path = './data/images/22_Picnic_Picnic_22_10.jpg'\n",
    "\n",
    "model_inference_image_list_wo_nms(model_path=onnx_path, img_path = img_path, mean=0, scale=0.0039, dst_path='./onnx_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d626d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
